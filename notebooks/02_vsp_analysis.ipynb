{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VSP Analysis with DAS Data\n",
    "\n",
    "This notebook demonstrates Vertical Seismic Profiling (VSP) processing workflows for Distributed Acoustic Sensing (DAS) data.\n",
    "\n",
    "## Overview\n",
    "\n",
    "VSP is a seismic technique where sensors are placed at various depths in a borehole to record seismic waves generated by a surface source. When combined with DAS technology, fiber optic cables provide high-resolution, continuous measurements along the entire wellbore.\n",
    "\n",
    "### Key VSP Processing Steps:\n",
    "1. Data loading and quality control\n",
    "2. First break picking\n",
    "3. Upgoing/downgoing wave separation (median filtering)\n",
    "4. Corridor stack generation\n",
    "5. Velocity analysis\n",
    "6. Well-to-seismic tie\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand DAS-VSP data characteristics\n",
    "- Implement wavefield separation techniques\n",
    "- Generate corridor stacks for well ties\n",
    "- Handle real-world data quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal, interpolate\n",
    "from scipy.ndimage import median_filter\n",
    "import h5py\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting parameters\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Synthetic VSP Data Generation\n",
    "\n",
    "For demonstration purposes, we'll create synthetic DAS-VSP data that mimics real acquisition scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_vsp_data(n_channels=500, n_samples=2000, fs=1000, \n",
    "                                channel_spacing=1.0, velocity=3000):\n",
    "    \"\"\"\n",
    "    Generate synthetic DAS-VSP data with downgoing and upgoing waves.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_channels : int\n",
    "        Number of depth channels (DAS channels along fiber)\n",
    "    n_samples : int\n",
    "        Number of time samples\n",
    "    fs : float\n",
    "        Sampling frequency in Hz\n",
    "    channel_spacing : float\n",
    "        Spatial sampling in meters\n",
    "    velocity : float\n",
    "        Wave propagation velocity in m/s\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    data : ndarray\n",
    "        Synthetic VSP data (n_channels x n_samples)\n",
    "    time : ndarray\n",
    "        Time axis in seconds\n",
    "    depth : ndarray\n",
    "        Depth axis in meters\n",
    "    \"\"\"\n",
    "    # Create axes\n",
    "    time = np.arange(n_samples) / fs\n",
    "    depth = np.arange(n_channels) * channel_spacing\n",
    "    \n",
    "    # Initialize data array\n",
    "    data = np.zeros((n_channels, n_samples))\n",
    "    \n",
    "    # Create source wavelet (Ricker wavelet)\n",
    "    f_peak = 50  # Hz\n",
    "    t_wavelet = np.arange(-0.05, 0.05, 1/fs)\n",
    "    wavelet = (1 - 2*(np.pi*f_peak*t_wavelet)**2) * np.exp(-(np.pi*f_peak*t_wavelet)**2)\n",
    "    \n",
    "    # Add downgoing wave\n",
    "    for i, d in enumerate(depth):\n",
    "        travel_time = d / velocity\n",
    "        arrival_sample = int(travel_time * fs) + 200  # offset for visualization\n",
    "        \n",
    "        if arrival_sample < n_samples - len(wavelet):\n",
    "            data[i, arrival_sample:arrival_sample+len(wavelet)] += wavelet * np.exp(-d/2000)\n",
    "    \n",
    "    # Add upgoing reflections (from multiple reflectors)\n",
    "    reflector_depths = [1500, 2500, 3500]  # meters\n",
    "    reflector_coeffs = [-0.3, 0.5, -0.4]    # reflection coefficients\n",
    "    \n",
    "    for refl_depth, refl_coef in zip(reflector_depths, reflector_coeffs):\n",
    "        for i, d in enumerate(depth):\n",
    "            if d < refl_depth:\n",
    "                # Two-way travel time\n",
    "                travel_time = (2*refl_depth - d) / velocity\n",
    "                arrival_sample = int(travel_time * fs) + 200\n",
    "                \n",
    "                if arrival_sample < n_samples - len(wavelet):\n",
    "                    data[i, arrival_sample:arrival_sample+len(wavelet)] += \\\n",
    "                        wavelet * refl_coef * np.exp(-refl_depth/2000)\n",
    "    \n",
    "    # Add realistic noise\n",
    "    noise_level = 0.05\n",
    "    data += np.random.randn(n_channels, n_samples) * noise_level\n",
    "    \n",
    "    # Add coherent noise (tube waves)\n",
    "    tube_wave_velocity = 1500  # m/s (slower than formation)\n",
    "    for i, d in enumerate(depth):\n",
    "        travel_time = d / tube_wave_velocity\n",
    "        arrival_sample = int(travel_time * fs) + 200\n",
    "        \n",
    "        if arrival_sample < n_samples - len(wavelet):\n",
    "            tube_wavelet = wavelet * 0.3 * np.exp(-d/1000)\n",
    "            data[i, arrival_sample:arrival_sample+len(wavelet)] += tube_wavelet\n",
    "    \n",
    "    return data, time, depth\n",
    "\n",
    "# Generate synthetic VSP data\n",
    "vsp_data, time_axis, depth_axis = generate_synthetic_vsp_data(\n",
    "    n_channels=500, \n",
    "    n_samples=2000, \n",
    "    fs=1000,\n",
    "    channel_spacing=1.0,\n",
    "    velocity=3000\n",
    ")\n",
    "\n",
    "print(f\"VSP Data shape: {vsp_data.shape}\")\n",
    "print(f\"Time range: {time_axis[0]:.3f} - {time_axis[-1]:.3f} s\")\n",
    "print(f\"Depth range: {depth_axis[0]:.1f} - {depth_axis[-1]:.1f} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Visualization and Quality Control\n",
    "\n",
    "First step in any VSP processing workflow is to visualize the raw data and assess its quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vsp_data(data, time, depth, title=\"VSP Data\", vmin=None, vmax=None, cmap='seismic'):\n",
    "    \"\"\"\n",
    "    Plot VSP data with proper scaling and labels.\n",
    "    \"\"\"\n",
    "    if vmin is None:\n",
    "        vmin = -np.percentile(np.abs(data), 98)\n",
    "    if vmax is None:\n",
    "        vmax = np.percentile(np.abs(data), 98)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    im = ax.imshow(data, aspect='auto', cmap=cmap, \n",
    "                   extent=[time[0]*1000, time[-1]*1000, depth[-1], depth[0]],\n",
    "                   vmin=vmin, vmax=vmax, interpolation='bilinear')\n",
    "    \n",
    "    ax.set_xlabel('Time (ms)', fontsize=12)\n",
    "    ax.set_ylabel('Depth (m)', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.colorbar(im, ax=ax, label='Amplitude')\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "# Plot raw VSP data\n",
    "fig, ax = plot_vsp_data(vsp_data, time_axis, depth_axis, title=\"Raw DAS-VSP Data\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Raw data visualization complete\")\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Downgoing direct wave (linear moveout from top-left)\")\n",
    "print(\"- Upgoing reflections (opposite moveout)\")\n",
    "print(\"- Tube waves (slower linear events)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. First Break Picking\n",
    "\n",
    "First breaks mark the arrival time of the downgoing direct wave at each depth. This is crucial for:\n",
    "- Velocity analysis\n",
    "- Time-to-depth conversion\n",
    "- Wavefield separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_first_breaks(data, time, depth, search_window=(0.15, 0.8), \n",
    "                      method='energy_ratio'):\n",
    "    \"\"\"\n",
    "    Automatic first break picking using energy ratio method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : ndarray\n",
    "        VSP data (n_channels x n_samples)\n",
    "    time : ndarray\n",
    "        Time axis\n",
    "    depth : ndarray\n",
    "        Depth axis\n",
    "    search_window : tuple\n",
    "        (start_time, end_time) in seconds for searching\n",
    "    method : str\n",
    "        Picking method ('energy_ratio', 'sta_lta', 'correlation')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    first_breaks : ndarray\n",
    "        First break times for each channel\n",
    "    \"\"\"\n",
    "    n_channels = data.shape[0]\n",
    "    first_breaks = np.zeros(n_channels)\n",
    "    \n",
    "    # Find search window indices\n",
    "    dt = time[1] - time[0]\n",
    "    start_idx = int(search_window[0] / dt)\n",
    "    end_idx = int(search_window[1] / dt)\n",
    "    \n",
    "    if method == 'energy_ratio':\n",
    "        for i in range(n_channels):\n",
    "            trace = data[i, start_idx:end_idx]\n",
    "            \n",
    "            # Calculate energy ratio\n",
    "            window_len = 20\n",
    "            energy_ratio = np.zeros(len(trace) - 2*window_len)\n",
    "            \n",
    "            for j in range(len(energy_ratio)):\n",
    "                pre_energy = np.sum(trace[j:j+window_len]**2)\n",
    "                post_energy = np.sum(trace[j+window_len:j+2*window_len]**2)\n",
    "                energy_ratio[j] = post_energy / (pre_energy + 1e-10)\n",
    "            \n",
    "            # Pick maximum energy ratio\n",
    "            pick_idx = np.argmax(energy_ratio) + window_len + start_idx\n",
    "            first_breaks[i] = time[pick_idx]\n",
    "    \n",
    "    return first_breaks\n",
    "\n",
    "# Pick first breaks\n",
    "first_breaks = pick_first_breaks(vsp_data, time_axis, depth_axis)\n",
    "\n",
    "# Calculate interval velocity\n",
    "interval_velocity = np.gradient(depth_axis) / np.gradient(first_breaks)\n",
    "\n",
    "# Plot first breaks and velocity\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot data with first breaks\n",
    "im = ax1.imshow(vsp_data, aspect='auto', cmap='seismic',\n",
    "                extent=[time_axis[0]*1000, time_axis[-1]*1000, depth_axis[-1], depth_axis[0]],\n",
    "                vmin=-np.percentile(np.abs(vsp_data), 98),\n",
    "                vmax=np.percentile(np.abs(vsp_data), 98))\n",
    "ax1.plot(first_breaks*1000, depth_axis, 'r-', linewidth=2, label='First Breaks')\n",
    "ax1.set_xlabel('Time (ms)')\n",
    "ax1.set_ylabel('Depth (m)')\n",
    "ax1.set_title('First Break Picks')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot interval velocity\n",
    "ax2.plot(interval_velocity, depth_axis, 'b-', linewidth=2)\n",
    "ax2.set_xlabel('Interval Velocity (m/s)')\n",
    "ax2.set_ylabel('Depth (m)')\n",
    "ax2.set_title('Interval Velocity from First Breaks')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ First breaks picked for {len(first_breaks)} channels\")\n",
    "print(f\"Average velocity: {np.mean(interval_velocity):.1f} m/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Wavefield Separation: Downgoing vs Upgoing\n",
    "\n",
    "One of the most critical steps in VSP processing is separating the downgoing waves from upgoing reflections.\n",
    "\n",
    "### Method: Median Filtering\n",
    "- Exploits the opposite moveout of upgoing and downgoing waves\n",
    "- Median filter in time-depth domain removes events with specific moveout\n",
    "- Filter length controls which events are separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_wavefields(data, filter_length=15):\n",
    "    \"\"\"\n",
    "    Separate upgoing and downgoing waves using median filtering.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : ndarray\n",
    "        Input VSP data (n_channels x n_samples)\n",
    "    filter_length : int\n",
    "        Median filter length (odd number)\n",
    "        - Smaller values: preserve more high-frequency content\n",
    "        - Larger values: stronger separation but may distort signals\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    downgoing : ndarray\n",
    "        Downgoing wavefield\n",
    "    upgoing : ndarray\n",
    "        Upgoing wavefield (reflections)\n",
    "    \"\"\"\n",
    "    # Ensure filter length is odd\n",
    "    if filter_length % 2 == 0:\n",
    "        filter_length += 1\n",
    "    \n",
    "    # Apply median filter along depth axis\n",
    "    # This preserves downgoing events (coherent in depth)\n",
    "    downgoing = median_filter(data, size=(filter_length, 1), mode='reflect')\n",
    "    \n",
    "    # Upgoing = Total - Downgoing\n",
    "    upgoing = data - downgoing\n",
    "    \n",
    "    return downgoing, upgoing\n",
    "\n",
    "# Separate wavefields\n",
    "downgoing_wave, upgoing_wave = separate_wavefields(vsp_data, filter_length=15)\n",
    "\n",
    "# Plot separated wavefields\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "titles = ['Raw VSP Data', 'Downgoing Wavefield', 'Upgoing Wavefield (Reflections)']\n",
    "datasets = [vsp_data, downgoing_wave, upgoing_wave]\n",
    "\n",
    "for ax, data, title in zip(axes, datasets, titles):\n",
    "    vmax = np.percentile(np.abs(data), 98)\n",
    "    im = ax.imshow(data, aspect='auto', cmap='seismic',\n",
    "                   extent=[time_axis[0]*1000, time_axis[-1]*1000, \n",
    "                          depth_axis[-1], depth_axis[0]],\n",
    "                   vmin=-vmax, vmax=vmax)\n",
    "    ax.set_xlabel('Time (ms)')\n",
    "    ax.set_ylabel('Depth (m)')\n",
    "    ax.set_title(title, fontweight='bold')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Wavefield separation complete\")\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"- Downgoing: Direct wave and tube waves\")\n",
    "print(\"- Upgoing: Reflection events from subsurface interfaces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Corridor Stack Generation\n",
    "\n",
    "The corridor stack is created by:\n",
    "1. Flattening the upgoing wavefield using first breaks\n",
    "2. Extracting a narrow time window (corridor) around zero time\n",
    "3. Stacking the traces within this corridor\n",
    "\n",
    "This produces a pseudo-reflection trace that can be compared with surface seismic data for well-to-seismic tie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corridor_stack(upgoing_data, time, depth, first_breaks, \n",
    "                           corridor_width=0.05, taper_width=0.01):\n",
    "    \"\"\"\n",
    "    Generate corridor stack from upgoing wavefield.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    upgoing_data : ndarray\n",
    "        Upgoing wavefield data\n",
    "    time : ndarray\n",
    "        Time axis\n",
    "    depth : ndarray\n",
    "        Depth axis\n",
    "    first_breaks : ndarray\n",
    "        First break times for each depth\n",
    "    corridor_width : float\n",
    "        Width of corridor in seconds (±)\n",
    "    taper_width : float\n",
    "        Taper width at edges in seconds\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    corridor_stack : ndarray\n",
    "        Stacked trace\n",
    "    two_way_time : ndarray\n",
    "        Two-way time axis\n",
    "    \"\"\"\n",
    "    n_channels, n_samples = upgoing_data.shape\n",
    "    dt = time[1] - time[0]\n",
    "    \n",
    "    # Create flattened data (align on first breaks)\n",
    "    flattened = np.zeros_like(upgoing_data)\n",
    "    \n",
    "    for i in range(n_channels):\n",
    "        # Calculate shift for this channel\n",
    "        shift_samples = int(first_breaks[i] / dt)\n",
    "        \n",
    "        if shift_samples < n_samples:\n",
    "            # Shift trace to align first break at t=0\n",
    "            flattened[i, :n_samples-shift_samples] = upgoing_data[i, shift_samples:]\n",
    "    \n",
    "    # Create corridor mute\n",
    "    corridor_samples = int(corridor_width / dt)\n",
    "    taper_samples = int(taper_width / dt)\n",
    "    \n",
    "    # Create Tukey window for corridor\n",
    "    mute = np.zeros(n_samples)\n",
    "    center = n_samples // 2\n",
    "    \n",
    "    # Corridor region\n",
    "    start = max(0, center - corridor_samples)\n",
    "    end = min(n_samples, center + corridor_samples)\n",
    "    mute[start:end] = 1.0\n",
    "    \n",
    "    # Apply taper\n",
    "    if taper_samples > 0:\n",
    "        taper_start = np.linspace(0, 1, taper_samples)\n",
    "        taper_end = np.linspace(1, 0, taper_samples)\n",
    "        \n",
    "        if start + taper_samples < end:\n",
    "            mute[start:start+taper_samples] = taper_start\n",
    "            mute[end-taper_samples:end] = taper_end\n",
    "    \n",
    "    # Apply corridor mute and stack\n",
    "    corridor_stack = np.zeros(n_samples)\n",
    "    weights = np.zeros(n_samples)\n",
    "    \n",
    "    for i in range(n_channels):\n",
    "        corridor_stack += flattened[i, :] * mute\n",
    "        weights += mute\n",
    "    \n",
    "    # Normalize by number of contributing traces\n",
    "    weights[weights == 0] = 1  # Avoid division by zero\n",
    "    corridor_stack /= weights\n",
    "    \n",
    "    # Create two-way time axis (centered on zero)\n",
    "    two_way_time = (np.arange(n_samples) - n_samples//2) * dt\n",
    "    \n",
    "    return corridor_stack, two_way_time, flattened\n",
    "\n",
    "# Generate corridor stack\n",
    "corridor_stack, twt_axis, flattened_data = generate_corridor_stack(\n",
    "    upgoing_wave, time_axis, depth_axis, first_breaks,\n",
    "    corridor_width=0.05, taper_width=0.01\n",
    ")\n",
    "\n",
    "# Plot corridor stack generation\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Upgoing wavefield\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "vmax = np.percentile(np.abs(upgoing_wave), 98)\n",
    "ax1.imshow(upgoing_wave, aspect='auto', cmap='seismic',\n",
    "           extent=[time_axis[0]*1000, time_axis[-1]*1000, depth_axis[-1], depth_axis[0]],\n",
    "           vmin=-vmax, vmax=vmax)\n",
    "ax1.plot(first_breaks*1000, depth_axis, 'r--', linewidth=2, label='First Breaks')\n",
    "ax1.set_xlabel('Time (ms)')\n",
    "ax1.set_ylabel('Depth (m)')\n",
    "ax1.set_title('Upgoing Wavefield', fontweight='bold')\n",
    "ax1.legend()\n",
    "\n",
    "# Flattened data\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "vmax = np.percentile(np.abs(flattened_data), 98)\n",
    "ax2.imshow(flattened_data, aspect='auto', cmap='seismic',\n",
    "           extent=[twt_axis[0]*1000, twt_axis[-1]*1000, depth_axis[-1], depth_axis[0]],\n",
    "           vmin=-vmax, vmax=vmax)\n",
    "ax2.axvline(0, color='r', linestyle='--', linewidth=2, label='Zero Time')\n",
    "ax2.set_xlabel('Two-Way Time (ms)')\n",
    "ax2.set_ylabel('Depth (m)')\n",
    "ax2.set_title('Flattened Upgoing Wavefield', fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "# Corridor stack\n",
    "ax3 = fig.add_subplot(gs[1, :])\n",
    "ax3.plot(twt_axis*1000, corridor_stack, 'b-', linewidth=2)\n",
    "ax3.fill_between(twt_axis*1000, 0, corridor_stack, alpha=0.3)\n",
    "ax3.axhline(0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax3.axvline(0, color='r', linestyle='--', linewidth=1)\n",
    "ax3.set_xlabel('Two-Way Time (ms)')\n",
    "ax3.set_ylabel('Amplitude')\n",
    "ax3.set_title('Corridor Stack (Pseudo-Reflection)', fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_xlim([twt_axis[0]*1000, twt_axis[-1]*1000])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Corridor stack generated successfully\")\n",
    "print(\"\\nThis trace can be used for:\")\n",
    "print(\"- Well-to-seismic tie\")\n",
    "print(\"- Wavelet extraction\")\n",
    "print(\"- Synthetic seismogram comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real-World Scenarios and Challenges\n",
    "\n",
    "### Scenario 1: Dealing with Poor Coupling\n",
    "\n",
    "**Problem**: DAS fiber may have poor coupling in certain depth intervals, leading to weak signals or gaps in data.\n",
    "\n",
    "**Solution**: Identify and interpolate over dead channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_dead_channels(data, threshold_percentile=5):\n",
    "    \"\"\"\n",
    "    Detect channels with anomalously low energy (poor coupling).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : ndarray\n",
    "        VSP data\n",
    "    threshold_percentile : float\n",
    "        Percentile threshold for dead channel detection\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dead_channels : ndarray (bool)\n",
    "        Boolean mask of dead channels\n",
    "    \"\"\"\n",
    "    # Calculate RMS energy for each channel\n",
    "    channel_energy = np.sqrt(np.mean(data**2, axis=1))\n",
    "    \n",
    "    # Identify channels below threshold\n",
    "    threshold = np.percentile(channel_energy, threshold_percentile)\n",
    "    dead_channels = channel_energy < threshold\n",
    "    \n",
    "    return dead_channels, channel_energy\n",
    "\n",
    "def interpolate_dead_channels(data, dead_channels):\n",
    "    \"\"\"\n",
    "    Interpolate over dead channels using neighboring traces.\n",
    "    \"\"\"\n",
    "    data_interp = data.copy()\n",
    "    n_channels = data.shape[0]\n",
    "    \n",
    "    # Find groups of consecutive dead channels\n",
    "    dead_idx = np.where(dead_channels)[0]\n",
    "    \n",
    "    for idx in dead_idx:\n",
    "        # Find nearest live channels\n",
    "        left = idx - 1\n",
    "        while left >= 0 and dead_channels[left]:\n",
    "            left -= 1\n",
    "        \n",
    "        right = idx + 1\n",
    "        while right < n_channels and dead_channels[right]:\n",
    "            right += 1\n",
    "        \n",
    "        # Interpolate\n",
    "        if left >= 0 and right < n_channels:\n",
    "            weight = (idx - left) / (right - left)\n",
    "            data_interp[idx] = (1 - weight) * data[left] + weight * data[right]\n",
    "        elif left >= 0:\n",
    "            data_interp[idx] = data[left]\n",
    "        elif right < n_channels:\n",
    "            data_interp[idx] = data[right]\n",
    "    \n",
    "    return data_interp\n",
    "\n",
    "# Simulate dead channels\n",
    "vsp_with_dead = vsp_data.copy()\n",
    "vsp_with_dead[100:110, :] *= 0.1  # Simulate poor coupling\n",
    "vsp_with_dead[250, :] *= 0.05     # Single dead channel\n",
    "\n",
    "# Detect and fix\n",
    "dead_mask, energy = detect_dead_channels(vsp_with_dead)\n",
    "vsp_fixed = interpolate_dead_channels(vsp_with_dead, dead_mask)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Original\n",
    "vmax = np.percentile(np.abs(vsp_data), 98)\n",
    "axes[0].imshow(vsp_data, aspect='auto', cmap='seismic',\n",
    "               extent=[time_axis[0]*1000, time_axis[-1]*1000, depth_axis[-1], depth_axis[0]],\n",
    "               vmin=-vmax, vmax=vmax)\n",
    "axes[0].set_title('Original Data')\n",
    "axes[0].set_xlabel('Time (ms)')\n",
    "axes[0].set_ylabel('Depth (m)')\n",
    "\n",
    "# With dead channels\n",
    "axes[1].imshow(vsp_with_dead, aspect='auto', cmap='seismic',\n",
    "               extent=[time_axis[0]*1000, time_axis[-1]*1000, depth_axis[-1], depth_axis[0]],\n",
    "               vmin=-vmax, vmax=vmax)\n",
    "axes[1].axhspan(depth_axis[100], depth_axis[110], alpha=0.3, color='red', \n",
    "                label='Dead zone')\n",
    "axes[1].axhline(depth_axis[250], color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_title('Data with Poor Coupling')\n",
    "axes[1].set_xlabel('Time (ms)')\n",
    "axes[1].legend()\n",
    "\n",
    "# Fixed\n",
    "axes[2].imshow(vsp_fixed, aspect='auto', cmap='seismic',\n",
    "               extent=[time_axis[0]*1000, time_axis[-1]*1000, depth_axis[-1], depth_axis[0]],\n",
    "               vmin=-vmax, vmax=vmax)\n",
    "axes[2].set_title('After Interpolation')\n",
    "axes[2].set_xlabel('Time (ms)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Detected {np.sum(dead_mask)} dead channels ({100*np.sum(dead_mask)/len(dead_mask):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2: Tube Wave Attenuation\n",
    "\n",
    "**Problem**: Tube waves are coherent noise traveling along the borehole, interfering with reflection signals.\n",
    "\n",
    "**Solution**: F-K filtering to remove coherent noise with specific velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fk_filter(data, dt, dx, velocity_min, velocity_max):\n",
    "    \"\"\"\n",
    "    Apply F-K (frequency-wavenumber) filter to remove coherent noise.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : ndarray\n",
    "        Input data (n_channels x n_samples)\n",
    "    dt : float\n",
    "        Time sampling interval (s)\n",
    "    dx : float\n",
    "        Spatial sampling interval (m)\n",
    "    velocity_min : float\n",
    "        Minimum velocity to pass (m/s)\n",
    "    velocity_max : float\n",
    "        Maximum velocity to pass (m/s)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    filtered_data : ndarray\n",
    "        Filtered data\n",
    "    \"\"\"\n",
    "    # 2D FFT to F-K domain\n",
    "    fk_data = np.fft.fft2(data)\n",
    "    fk_data = np.fft.fftshift(fk_data)\n",
    "    \n",
    "    # Create frequency and wavenumber axes\n",
    "    n_channels, n_samples = data.shape\n",
    "    freq = np.fft.fftshift(np.fft.fftfreq(n_samples, dt))\n",
    "    knum = np.fft.fftshift(np.fft.fftfreq(n_channels, dx))\n",
    "    \n",
    "    # Create 2D meshgrid\n",
    "    K, F = np.meshgrid(knum, freq, indexing='ij')\n",
    "    \n",
    "    # Calculate apparent velocity: v = f/k\n",
    "    # Avoid division by zero\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        velocity = np.abs(F / (K + 1e-10))\n",
    "    \n",
    "    # Create filter mask (pass velocities outside tube wave range)\n",
    "    filter_mask = np.ones_like(velocity)\n",
    "    filter_mask[(velocity >= velocity_min) & (velocity <= velocity_max)] = 0\n",
    "    \n",
    "    # Apply taper to avoid ringing\n",
    "    taper_width = 0.1  # 10% of velocity range\n",
    "    v_range = velocity_max - velocity_min\n",
    "    \n",
    "    # Smooth transition\n",
    "    transition_zone = (velocity >= velocity_min - taper_width * v_range) & \\\n",
    "                     (velocity <= velocity_min + taper_width * v_range)\n",
    "    filter_mask[transition_zone] = 0.5 * (1 + np.cos(\n",
    "        np.pi * (velocity[transition_zone] - velocity_min) / (taper_width * v_range)))\n",
    "    \n",
    "    transition_zone = (velocity >= velocity_max - taper_width * v_range) & \\\n",
    "                     (velocity <= velocity_max + taper_width * v_range)\n",
    "    filter_mask[transition_zone] = 0.5 * (1 + np.cos(\n",
    "        np.pi * (velocity_max - velocity[transition_zone]) / (taper_width * v_range)))\n",
    "    \n",
    "    # Apply filter\n",
    "    fk_filtered = fk_data * filter_mask\n",
    "    \n",
    "    # Inverse FFT back to space-time domain\n",
    "    fk_filtered = np.fft.ifftshift(fk_filtered)\n",
    "    filtered_data = np.real(np.fft.ifft2(fk_filtered))\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "# Apply F-K filter to remove tube waves (1200-1800 m/s)\n",
    "dt = time_axis[1] - time_axis[0]\n",
    "dx = depth_axis[1] - depth_axis[0]\n",
    "\n",
    "vsp_fk_filtered = fk_filter(vsp_data, dt, dx, \n",
    "                            velocity_min=1200, velocity_max=1800)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "vmax = np.percentile(np.abs(vsp_data), 98)\n",
    "\n",
    "axes[0].imshow(vsp_data, aspect='auto', cmap='seismic',\n",
    "               extent=[time_axis[0]*1000, time_axis[-1]*1000, depth_axis[-1], depth_axis[0]],\n",
    "               vmin=-vmax, vmax=vmax)\n",
    "axes[0].set_title('Before F-K Filter')\n",
    "axes[0].set_xlabel('Time (ms)')\n",
    "axes[0].set_ylabel('Depth (m)')\n",
    "\n",
    "axes[1].imshow(vsp_fk_filtered, aspect='auto', cmap='seismic',\n",
    "               extent=[time_axis[0]*1000, time_axis[-1]*1000, depth_axis[-1], depth_axis[0]],\n",
    "               vmin=-vmax, vmax=vmax)\n",
    "axes[1].set_title('After F-K Filter (Tube Waves Removed)')\n",
    "axes[1].set_xlabel('Time (ms)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ F-K filter applied to attenuate tube waves (1200-1800 m/s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Troubleshooting Guide\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "#### Issue 1: Weak or Missing First Breaks\n",
    "**Symptoms**: \n",
    "- Automatic picking fails\n",
    "- Irregular first break times\n",
    "\n",
    "**Possible Causes**:\n",
    "- Poor signal-to-noise ratio\n",
    "- Incorrect search window\n",
    "- Source coupling issues\n",
    "\n",
    "**Solutions**:\n",
    "1. Apply bandpass filter before picking (20-100 Hz typical)\n",
    "2. Adjust search window based on expected velocities\n",
    "3. Use correlation-based picking with reference trace\n",
    "4. Manual quality control and correction\n",
    "\n",
    "#### Issue 2: Poor Wavefield Separation\n",
    "**Symptoms**:\n",
    "- Residual downgoing energy in upgoing section\n",
    "- Distorted reflection signals\n",
    "\n",
    "**Possible Causes**:\n",
    "- Inappropriate median filter length\n",
    "- Complex velocity structure\n",
    "- Multiple tube wave modes\n",
    "\n",
    "**Solutions**:\n",
    "1. Test different filter lengths (typically 9-21)\n",
    "2. Apply F-K filtering before median filtering\n",
    "3. Use adaptive filters based on local dip\n",
    "4. Consider tau-p domain separation\n",
    "\n",
    "#### Issue 3: Noisy Corridor Stack\n",
    "**Symptoms**:\n",
    "- Low signal-to-noise in corridor stack\n",
    "- Difficult well-to-seismic tie\n",
    "\n",
    "**Possible Causes**:\n",
    "- Incomplete wavefield separation\n",
    "- Corridor window too narrow/wide\n",
    "- Poor data quality in certain depth ranges\n",
    "\n",
    "**Solutions**:\n",
    "1. Optimize corridor width (typically 40-100 ms)\n",
    "2. Apply depth-dependent weighting\n",
    "3. Exclude poor-quality depth intervals\n",
    "4. Apply spectral balancing\n",
    "\n",
    "#### Issue 4: DAS-Specific Challenges\n",
    "**Symptoms**:\n",
    "- Spatially varying sensitivity\n",
    "- Fading or signal dropout\n",
    "\n",
    "**DAS-Specific Considerations**:\n",
    "1. **Gauge length effects**: \n",
    "   - DAS measures strain rate over gauge length (typically 10m)\n",
    "   - Acts as spatial averaging → reduces resolution\n",
    "   - Notch frequencies at wavelengths = gauge length\n",
    "\n",
    "2. **Coupling variations**:\n",
    "   - Cement quality affects signal strength\n",
    "   - Temperature gradients affect fiber response\n",
    "   - Solution: Amplitude normalization by depth\n",
    "\n",
    "3. **Directional sensitivity**:\n",
    "   - DAS measures axial strain only\n",
    "   - Horizontal fiber → poor P-wave response\n",
    "   - Vertical fiber → optimal for VSP\n",
    "\n",
    "### Quality Control Checklist\n",
    "\n",
    "- [ ] Check data acquisition parameters (sample rate, gauge length, spacing)\n",
    "- [ ] Verify depth calibration against known markers\n",
    "- [ ] Inspect raw data for obvious issues (noise, dead channels)\n",
    "- [ ] Validate first break picks (should be smooth with depth)\n",
    "- [ ] Check interval velocities (should match expected geology)\n",
    "- [ ] Verify wavefield separation quality visually\n",
    "- [ ] Ensure corridor stack shows coherent reflections\n",
    "- [ ] Compare with offset VSP or surface seismic if available\n",
    "- [ ] Document all processing parameters for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete Processing Workflow Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vsp_workflow(raw_data, time, depth, \n",
    "                        apply_fk=True, \n",
    "                        median_length=15,\n",
    "                        corridor_width=0.05):\n",
    "    \"\"\"\n",
    "    Complete VSP processing workflow.\n",
    "    \n",
    "    Returns all intermediate and final products.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"VSP Processing Workflow\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Quality control\n",
    "    print(\"[1/6] Quality control and dead channel detection...\")\n",
    "    dead_channels, energy = detect_dead_channels(raw_data)\n",
    "    data_qc = interpolate_dead_channels(raw_data, dead_channels)\n",
    "    results['data_qc'] = data_qc\n",
    "    print(f\"      Found and fixed {np.sum(dead_channels)} dead channels\")\n",
    "    \n",
    "    # Step 2: F-K filtering (optional)\n",
    "    if apply_fk:\n",
    "        print(\"[2/6] F-K filtering to remove tube waves...\")\n",
    "        dt = time[1] - time[0]\n",
    "        dx = depth[1] - depth[0]\n",
    "        data_fk = fk_filter(data_qc, dt, dx, 1200, 1800)\n",
    "        results['data_fk'] = data_fk\n",
    "        print(\"      Tube waves attenuated\")\n",
    "    else:\n",
    "        data_fk = data_qc\n",
    "        print(\"[2/6] Skipping F-K filter\")\n",
    "    \n",
    "    # Step 3: First break picking\n",
    "    print(\"[3/6] First break picking...\")\n",
    "    fb = pick_first_breaks(data_fk, time, depth)\n",
    "    results['first_breaks'] = fb\n",
    "    print(\"      First breaks picked\")\n",
    "    \n",
    "    # Step 4: Wavefield separation\n",
    "    print(\"[4/6] Wavefield separation...\")\n",
    "    downgoing, upgoing = separate_wavefields(data_fk, median_length)\n",
    "    results['downgoing'] = downgoing\n",
    "    results['upgoing'] = upgoing\n",
    "    print(\"      Upgoing and downgoing waves separated\")\n",
    "    \n",
    "    # Step 5: Corridor stack\n",
    "    print(\"[5/6] Generating corridor stack...\")\n",
    "    corridor, twt, flattened = generate_corridor_stack(\n",
    "        upgoing, time, depth, fb, corridor_width\n",
    "    )\n",
    "    results['corridor_stack'] = corridor\n",
    "    results['two_way_time'] = twt\n",
    "    results['flattened'] = flattened\n",
    "    print(\"      Corridor stack generated\")\n",
    "    \n",
    "    # Step 6: Velocity analysis\n",
    "    print(\"[6/6] Velocity analysis...\")\n",
    "    interval_vel = np.gradient(depth) / np.gradient(fb)\n",
    "    results['interval_velocity'] = interval_vel\n",
    "    print(f\"      Average velocity: {np.mean(interval_vel):.1f} m/s\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Processing complete!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run complete workflow\n",
    "processed = process_vsp_workflow(\n",
    "    vsp_data, time_axis, depth_axis,\n",
    "    apply_fk=True,\n",
    "    median_length=15,\n",
    "    corridor_width=0.05\n",
    ")\n",
    "\n",
    "print(\"\\nProcessed results available:\")\n",
    "for key in processed.keys():\n",
    "    print(f\"  - {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **DAS-VSP advantages**:\n",
    "   - High spatial sampling density\n",
    "   - Continuous coverage (no gaps)\n",
    "   - Permanent monitoring capability\n",
    "   - Cost-effective for long wells\n",
    "\n",
    "2. **Critical processing steps**:\n",
    "   - Quality control (dead channels, coupling)\n",
    "   - First break picking (foundation for everything)\n",
    "   - Wavefield separation (median filtering)\n",
    "   - Corridor stack (for well ties)\n",
    "\n",
    "3. **Common pitfalls to avoid**:\n",
    "   - Blindly trusting automatic picks\n",
    "   - Over-filtering (destroys signal)\n",
    "   - Ignoring DAS-specific effects (gauge length)\n",
    "   - Poor corridor window selection\n",
    "\n",
    "4. **Best practices**:\n",
    "   - Always visualize intermediate results\n",
    "   - Document all processing parameters\n",
    "   - Compare with independent data (logs, surface seismic)\n",
    "   - Perform sensitivity analysis on key parameters\n",
    "   - Use domain knowledge (expected velocities, geology)\n",
    "\n",
    "### Further Reading:\n",
    "- Mateeva et al. (2014): \"Distributed acoustic sensing for reservoir monitoring\"\n",
    "- Daley et al. (2016): \"Field testing of fiber-optic distributed acoustic sensing\"\n",
    "- Hardage (2000): \"Vertical Seismic Profiling: Principles\" (classic reference)\n",
    "\n",
    "### Next Steps:\n",
    "- Apply to real field data\n",
    "- Integrate with petrophysical analysis\n",
    "- Time-lapse VSP for reservoir monitoring\n",
    "- 3D VSP for imaging around wellbore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
